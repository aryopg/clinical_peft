model_configs:
  peft_type: "adalora"
  task_type: "causal_lm"
  model_name_or_path: "axiong/PMC_LLaMA_13B"
  peft_hyperparameters:
    method: grid
    metric:
      name: eval_ppl
      goal: minimize
    parameters:
      target_r:
        values: [8]
      init_r:
        values: [12]
      tinit:
        values: [0, 600, 2000]
      tfinal:
        values: [0, 8000, 25000, 50000]
      deltaT:
        values: [1, 10, 100]
      beta1:
        values: [0.85]
      beta2:
        values: [0.85]
      orth_reg_weight:
        values: [0.5]
  model_hyperparameters:
    learning_rate: 0.0003
    warmup_steps_ratio: 0.06
    max_seq_len: 512
    gradient_accumulation_steps: 64
training_configs:
  dataset_paths:
    - aryopg/mimic-iv
  random_seed: 1234
  device: 0
  num_process: 8
  test_size: 0.1
  epochs: 1
  steps: 1000000
  batch_size: 4
  log_steps: 100
  eval_steps: 1000000
  checkpoint_steps: 1000000  # Only checkpoint at the end
  max_sweep_count: 20
  outputs_dir: "outputs"
