model_configs:
  peft_type: "lora"
  task_type: "seq_cls"
  model_name_or_path: "chaoyi-wu/PMC_LLAMA_7B"
  pretrained_peft_name_or_path: "aryopg/mimic-iv__PMC_LLAMA_7B__lora__lora_alpha_4__lora_dropout_0.2__r_4"
  pretrained_peft_fine_tune: false
  downstream_peft: true
  peft_hyperparameters:
    method: bayes
    metric:
      name: val_roc_auc
      goal: maximize
    parameters:
      r:
        values: [2, 4, 8, 16]
      lora_alpha:
        values: [4, 8, 16, 32]
      lora_dropout: 
        values: [0.0, 0.1, 0.2]
  model_hyperparameters:
    bf16: True
    learning_rate: 0.00005
    warmup_steps_ratio: 0.06
    max_seq_len: 512
    gradient_accumulation_steps: 10
training_configs:
  dataset_paths:
    - aryopg/trec-pmv
  random_seed: 1234
  device: 0
  num_process: 8
  test_size: 0.1
  epochs: 10
  steps: 1000000
  batch_size: 8
  log_steps: 1000000
  eval_steps: 1000000  # Only evaluate at the end
  checkpoint_steps: 1000000  # Only checkpoint at the end
  max_sweep_count: 10
  outputs_dir: "outputs"
